{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First part: Comparison of classifiers on simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_circles, make_classification, make_blobs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are two useful functions for plotting a dataset (only training, or all data split into training and test) and the decision boundary of a model and the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_dataset(X_train, y_train, X_test=None, y_test=None):\n",
    "    # -- function that plots the datapoints\n",
    "    h = 0.02 # -- h is the step length\n",
    "    x_min, x_max = X_train[:, 0].min() - 0.5, X_train[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_train[:, 1].min() - 0.5, X_train[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # -- just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
    "    ax = plt.subplot(1,1,1)\n",
    "    ax.set_title(\"Input data\")\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n",
    "    \n",
    "    if X_test is not None and y_test is not None:\n",
    "        # -- Plot the testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.2, edgecolors=\"k\")\n",
    "    \n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_model(input_model, X_train, y_train, X_test, y_test):\n",
    "    # -- function that plots the datapoints and decision boundaries of input_model\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_train[:, 0].min() - 0.5, X_train[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_train[:, 1].min() - 0.5, X_train[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # -- just plot the dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
    "    ax = plt.subplot(1, 1, 1)\n",
    "    \n",
    "    ax.set_title(\"Model decision boundary\")\n",
    "    # -- Plot the decision boundary. For that, we will assign a color to each\n",
    "    # -- point in the mesh [x_min, x_max] x [y_min, y_max].\n",
    "    if hasattr(input_model, \"decision_function\"):\n",
    "        Z = input_model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:\n",
    "        Z = input_model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "    # -- Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, cmap=cm, alpha = 0.8)\n",
    "\n",
    "    # -- Plot the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c = y_train, cmap = cm_bright, edgecolors = \"k\")\n",
    "    # -- Plot the testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c = y_test, cmap = cm_bright, edgecolors = \"k\", alpha = 0.2)\n",
    "\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate an almost linearly separable dataset and run the perceptron first, than SVM, then a NN with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- generate a random n-classification dataset\n",
    "X, y = make_classification(\n",
    "    n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1\n",
    ")\n",
    "\n",
    "# -- add noise to points exploiting a uniform distribution\n",
    "# -- the aim is to get closer to a non-linearly separable dataset \n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size = X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_dataset(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now print all data (i.e., train and and test). The points in the test set are the most transparent that will be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_dataset(X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's learn a perceptron, plot its decision boundary, and print the train error and the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "perceptron = Perceptron(random_state = 11)\n",
    "perceptron.fit(X_train_scaled, y_train)\n",
    "\n",
    "plot_model(perceptron, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "print(f'Training error:, {(1.0 - perceptron.score(X_train_scaled, y_train)):.5f}')\n",
    "\n",
    "print(f'Test error:, {(1.0 - perceptron.score(X_test_scaled, y_test)):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel = \"linear\", C = 1)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "plot_model(svm, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "print(f'Training error: {(1.0 - svm.score(X_train_scaled, y_train)):.5f}')\n",
    "print(f'Test error: {(1.0 - svm.score(X_test_scaled, y_test)):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with a NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- one hidden layer with size= 100, activation function = ReLU (see documentation)\n",
    "mlp = MLPClassifier(max_iter = 1000)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "plot_model(mlp, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "print(f'Training error: {(1.0 - mlp.score(X_train_scaled, y_train)):.5f}')\n",
    "print(f'Test error: {(1.0 - mlp.score(X_test_scaled, y_test)):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's try now with some more complex dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(noise = 0.3, random_state = 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_dataset(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_dataset(X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "perceptron = Perceptron(random_state = 11)\n",
    "perceptron.fit(X_train_scaled, y_train)\n",
    "\n",
    "plot_model(perceptron, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "print(f'Training error: {(1.0 - perceptron.score(X_train_scaled, y_train)):.5f}')\n",
    "print(f'Test error: {(1.0 - perceptron.score(X_test_scaled, y_test)):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel = \"linear\")\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "plot_model(svm, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "print(f'Training error: {(1.0 - svm.score(X_train_scaled, y_train)):.5f}')\n",
    "print(f'Test error: {(1.0 - svm.score(X_test_scaled, y_test)):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter = 1500)\n",
    "# -- Note that with max_iter = 1000 the model is not converging. (see 'tol' parameter). Try to re-train with max_iter = 1500\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "plot_model(mlp, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "print(f'Training error: {(1.0 - mlp.score(X_train_scaled, y_train)):.5f}')\n",
    "print(f'Test error: {(1.0 - mlp.score(X_test_scaled, y_test)):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Another interesting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = make_circles(noise = 0.2, factor = 0.5, random_state = 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_dataset(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_dataset(X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "perceptron = Perceptron(random_state = 11)\n",
    "perceptron.fit(X_train_scaled, y_train)\n",
    "\n",
    "plot_model(perceptron, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "print(f'Training error: {(1.0 - perceptron.score(X_train_scaled, y_train)):.5f}')\n",
    "print(f'Test error: {(1.0 - perceptron.score(X_test_scaled, y_test)):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel = \"linear\")\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "plot_model(svm, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "print(f'Training error: {(1.0 - svm.score(X_train_scaled, y_train)):.5f}')\n",
    "print(f'Test error: {(1.0 - svm.score(X_test_scaled, y_test)):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter = 1000)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "plot_model(mlp, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "print(f'Training error: {(1.0 - mlp.score(X_train_scaled, y_train)):.5f}')\n",
    "print(f'Test error: {(1.0 - mlp.score(X_test_scaled, y_test)):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's now consider the blobs dataset considered in the last Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- make_blobs dataset\n",
    "\n",
    "# -- generate the dataset\n",
    "X, y = make_blobs(n_samples = 1000, centers = 2, n_features = 2, center_box=(-7.5, 7.5), random_state = 37, cluster_std = 2.8)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# -- scale data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- perceptron\n",
    "perceptron = Perceptron(random_state = 11)\n",
    "perceptron.fit(X_train_scaled, y_train)\n",
    "\n",
    "plot_model(perceptron, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "print(f'Training error: {(1.0 - perceptron.score(X_train_scaled, y_train)):.5f}')\n",
    "print(f'Test error: {(1.0 - perceptron.score(X_test_scaled, y_test)):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- svm\n",
    "svm = SVC(kernel = \"linear\")\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "plot_model(svm, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "print(f'Training error: {(1.0 - svm.score(X_train_scaled, y_train)):.5f}')\n",
    "print(f'Test error: {(1.0 - svm.score(X_test_scaled, y_test)):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- NN (mlp)\n",
    "mlp = MLPClassifier(max_iter = 1000)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "plot_model(mlp, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "print(f'Training error: {(1.0 - mlp.score(X_train_scaled, y_train)):.5f}')\n",
    "print(f'Test error: {(1.0 - mlp.score(X_test_scaled, y_test)):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second part: Regression on House Pricing Dataset\n",
    "We consider a reduced version of a dataset containing house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015.\n",
    "\n",
    "https://www.kaggle.com/harlfoxem/housesalesprediction\n",
    "\n",
    "For each house we know 18 house features (e.g., number of bedrooms, number of bathrooms, etc.) plus its price, that is what we would like to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- put here your ID_Number  (numero di matricola)\n",
    "numero_di_matricola = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all packages needed\n",
    "# %matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -- avoid convergence warnings from sklearn library\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data, remove data samples/points with missing values (NaN) and take a look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.164000e+03</td>\n",
       "      <td>3.164000e+03</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3.164000e+03</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.645240e+09</td>\n",
       "      <td>5.354358e+05</td>\n",
       "      <td>3.381163</td>\n",
       "      <td>2.071903</td>\n",
       "      <td>2070.027813</td>\n",
       "      <td>1.525054e+04</td>\n",
       "      <td>1.434893</td>\n",
       "      <td>0.009798</td>\n",
       "      <td>0.244311</td>\n",
       "      <td>3.459229</td>\n",
       "      <td>7.615676</td>\n",
       "      <td>1761.252212</td>\n",
       "      <td>308.775601</td>\n",
       "      <td>1967.489254</td>\n",
       "      <td>94.668774</td>\n",
       "      <td>98077.125158</td>\n",
       "      <td>47.557868</td>\n",
       "      <td>-122.212337</td>\n",
       "      <td>1982.544564</td>\n",
       "      <td>13176.302465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.854203e+09</td>\n",
       "      <td>3.809004e+05</td>\n",
       "      <td>0.895472</td>\n",
       "      <td>0.768212</td>\n",
       "      <td>920.251879</td>\n",
       "      <td>4.254457e+04</td>\n",
       "      <td>0.507792</td>\n",
       "      <td>0.098513</td>\n",
       "      <td>0.776298</td>\n",
       "      <td>0.682592</td>\n",
       "      <td>1.166324</td>\n",
       "      <td>815.934864</td>\n",
       "      <td>458.977904</td>\n",
       "      <td>28.095275</td>\n",
       "      <td>424.439427</td>\n",
       "      <td>54.172937</td>\n",
       "      <td>0.140789</td>\n",
       "      <td>0.139577</td>\n",
       "      <td>686.256670</td>\n",
       "      <td>25413.180755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000102e+06</td>\n",
       "      <td>7.500000e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>380.000000</td>\n",
       "      <td>6.490000e+02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>380.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1900.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98001.000000</td>\n",
       "      <td>47.177500</td>\n",
       "      <td>-122.514000</td>\n",
       "      <td>620.000000</td>\n",
       "      <td>660.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.199775e+09</td>\n",
       "      <td>3.150000e+05</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1430.000000</td>\n",
       "      <td>5.453750e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1190.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98032.000000</td>\n",
       "      <td>47.459575</td>\n",
       "      <td>-122.324250</td>\n",
       "      <td>1480.000000</td>\n",
       "      <td>5429.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.027701e+09</td>\n",
       "      <td>4.450000e+05</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1910.000000</td>\n",
       "      <td>8.000000e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1545.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1969.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98059.000000</td>\n",
       "      <td>47.572500</td>\n",
       "      <td>-122.226000</td>\n",
       "      <td>1830.000000</td>\n",
       "      <td>7873.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.358175e+09</td>\n",
       "      <td>6.402500e+05</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>1.122250e+04</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2150.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>1990.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98117.000000</td>\n",
       "      <td>47.680250</td>\n",
       "      <td>-122.124000</td>\n",
       "      <td>2360.000000</td>\n",
       "      <td>10408.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.839301e+09</td>\n",
       "      <td>5.350000e+06</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8010.000000</td>\n",
       "      <td>1.651359e+06</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>6720.000000</td>\n",
       "      <td>2620.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>98199.000000</td>\n",
       "      <td>47.777600</td>\n",
       "      <td>-121.315000</td>\n",
       "      <td>5790.000000</td>\n",
       "      <td>425581.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id         price     bedrooms    bathrooms  sqft_living  \\\n",
       "count  3.164000e+03  3.164000e+03  3164.000000  3164.000000  3164.000000   \n",
       "mean   4.645240e+09  5.354358e+05     3.381163     2.071903  2070.027813   \n",
       "std    2.854203e+09  3.809004e+05     0.895472     0.768212   920.251879   \n",
       "min    1.000102e+06  7.500000e+04     0.000000     0.000000   380.000000   \n",
       "25%    2.199775e+09  3.150000e+05     3.000000     1.500000  1430.000000   \n",
       "50%    4.027701e+09  4.450000e+05     3.000000     2.000000  1910.000000   \n",
       "75%    7.358175e+09  6.402500e+05     4.000000     2.500000  2500.000000   \n",
       "max    9.839301e+09  5.350000e+06     8.000000     6.000000  8010.000000   \n",
       "\n",
       "           sqft_lot       floors   waterfront         view    condition  \\\n",
       "count  3.164000e+03  3164.000000  3164.000000  3164.000000  3164.000000   \n",
       "mean   1.525054e+04     1.434893     0.009798     0.244311     3.459229   \n",
       "std    4.254457e+04     0.507792     0.098513     0.776298     0.682592   \n",
       "min    6.490000e+02     1.000000     0.000000     0.000000     1.000000   \n",
       "25%    5.453750e+03     1.000000     0.000000     0.000000     3.000000   \n",
       "50%    8.000000e+03     1.000000     0.000000     0.000000     3.000000   \n",
       "75%    1.122250e+04     2.000000     0.000000     0.000000     4.000000   \n",
       "max    1.651359e+06     3.500000     1.000000     4.000000     5.000000   \n",
       "\n",
       "             grade   sqft_above  sqft_basement     yr_built  yr_renovated  \\\n",
       "count  3164.000000  3164.000000    3164.000000  3164.000000   3164.000000   \n",
       "mean      7.615676  1761.252212     308.775601  1967.489254     94.668774   \n",
       "std       1.166324   815.934864     458.977904    28.095275    424.439427   \n",
       "min       3.000000   380.000000       0.000000  1900.000000      0.000000   \n",
       "25%       7.000000  1190.000000       0.000000  1950.000000      0.000000   \n",
       "50%       7.000000  1545.000000       0.000000  1969.000000      0.000000   \n",
       "75%       8.000000  2150.000000     600.000000  1990.000000      0.000000   \n",
       "max      12.000000  6720.000000    2620.000000  2015.000000   2015.000000   \n",
       "\n",
       "            zipcode          lat         long  sqft_living15     sqft_lot15  \n",
       "count   3164.000000  3164.000000  3164.000000    3164.000000    3164.000000  \n",
       "mean   98077.125158    47.557868  -122.212337    1982.544564   13176.302465  \n",
       "std       54.172937     0.140789     0.139577     686.256670   25413.180755  \n",
       "min    98001.000000    47.177500  -122.514000     620.000000     660.000000  \n",
       "25%    98032.000000    47.459575  -122.324250    1480.000000    5429.500000  \n",
       "50%    98059.000000    47.572500  -122.226000    1830.000000    7873.000000  \n",
       "75%    98117.000000    47.680250  -122.124000    2360.000000   10408.250000  \n",
       "max    98199.000000    47.777600  -121.315000    5790.000000  425581.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- load the dataset\n",
    "df = pd.read_csv('kc_house_data.csv', sep = ',')\n",
    "# -- remove the data samples with missing values (NaN)\n",
    "df = df.dropna() \n",
    "\n",
    "# -- see features_explained.pdf (if you want)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract input and output data. We want to predict the price by using features other than id as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of data: 3164\n"
     ]
    }
   ],
   "source": [
    "Data = df.values\n",
    "# -- m = number of input samples\n",
    "m = Data.shape[0]\n",
    "print(\"Amount of data:\",m)\n",
    "Y = Data[:m, 2]\n",
    "X = Data[:m, 3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "\n",
    "We split the data into 3 parts: one will be used for training and choosing the parameters, one for choosing among different models, and one for testing. The part for training and choosing the parameters will consist of $2/3$ of all samples, the one for choosing among different models will consist of $1/6$ of all samples, while the other part consists of the remaining $1/6$-th of all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of data for training and deciding parameters: 2109\n",
      "Amount of data for validation (choosing among different models): 527\n",
      "Amount of data for test: 528\n"
     ]
    }
   ],
   "source": [
    "# -- Split data into train (2/3 of samples), validation (1/6 of samples), and test data (the rest)\n",
    "m_train = int(2./3.*m)\n",
    "m_val = int((m-m_train)/2.)\n",
    "m_test = m - m_train - m_val\n",
    "print(\"Amount of data for training and deciding parameters:\", m_train)\n",
    "print(\"Amount of data for validation (choosing among different models):\", m_val)\n",
    "print(\"Amount of data for test:\", m_test)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_and_val, X_test, Y_train_and_val, Y_test = train_test_split(X, Y, test_size = m_test/m, random_state = numero_di_matricola)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_and_val, Y_train_and_val, \n",
    "                                                  test_size = m_val/(m_train + m_val), random_state = numero_di_matricola)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's standardize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Data pre-processing\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train_and_val_scaled = scaler.transform(X_train_and_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "Let's start by learning a simple neural network with 1 hidden node.\n",
    "Note: we are going to use the input parameter solver='lbfgs' and random_state=numero_di_matricola to fix the random seed (so results are reproducible)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hereby define a function to train an MLPRegressor on the (already scaled) training data and (optionally) print its parameters at the end of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- look at kwargs** in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "def train_model(X_train, Y_train, X_val, Y_val, print_weights = True, **params):\n",
    "\n",
    "    mlp_model = MLPRegressor(**params)\n",
    "    mlp_model.fit(X_train, Y_train)\n",
    "\n",
    "    # -- let's print the error (1 - R^2) on training data\n",
    "    print(f'Training error: {(1.0 - mlp_model.score(X_train, Y_train)):.5f}')\n",
    "    # -- let's print the error (1 - R^2) on validation data\n",
    "    print(f'Validation error: {(1.0 - mlp_model.score(X_val, Y_val)):.5f}')\n",
    "\n",
    "    if print_weights:\n",
    "\n",
    "        weights = mlp_model.coefs_\n",
    "        biases = mlp_model.intercepts_\n",
    "    \n",
    "        # -- let's print the coefficients of the model for the input nodes (but not the bias)\n",
    "        print('\\n--- Weights of NN ---')\n",
    "    \n",
    "        for i_layer, (w, b) in enumerate(zip(weights, biases)):\n",
    "            print(f'\\n# Layer {i_layer+1}')\n",
    "            print(f'--- Weights, with shape {w.shape} ---')\n",
    "            for i in range(w.shape[0]):\n",
    "                for j in range(w.shape[1]):\n",
    "                    print(f'w_({i+1}, {j+1})^({i_layer+1}): {w[i][j]:.3f}')\n",
    "                    \n",
    "            print(f'--- Biases, with shape {b.shape} ---')\n",
    "            for i in range(b.shape[0]):\n",
    "                print(f'b_{i+1}: {b[i]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.26395\n",
      "Validation error: 0.30405\n",
      "\n",
      "--- Weights of NN ---\n",
      "\n",
      "# Layer 1\n",
      "--- Weights, with shape (18, 1) ---\n",
      "w_(1, 1)^(1): -214.895\n",
      "w_(2, 1)^(1): 269.680\n",
      "w_(3, 1)^(1): 524.770\n",
      "w_(4, 1)^(1): -60.813\n",
      "w_(5, 1)^(1): 4.051\n",
      "w_(6, 1)^(1): 712.429\n",
      "w_(7, 1)^(1): 294.930\n",
      "w_(8, 1)^(1): 136.762\n",
      "w_(9, 1)^(1): 817.570\n",
      "w_(10, 1)^(1): 494.134\n",
      "w_(11, 1)^(1): 163.890\n",
      "w_(12, 1)^(1): -583.215\n",
      "w_(13, 1)^(1): 38.036\n",
      "w_(14, 1)^(1): -203.947\n",
      "w_(15, 1)^(1): 601.258\n",
      "w_(16, 1)^(1): -142.476\n",
      "w_(17, 1)^(1): 147.377\n",
      "w_(18, 1)^(1): -27.049\n",
      "--- Biases, with shape (1,) ---\n",
      "b_1: 3801.748\n",
      "\n",
      "# Layer 2\n",
      "--- Weights, with shape (1, 1) ---\n",
      "w_(1, 1)^(2): 140.858\n",
      "--- Biases, with shape (1,) ---\n",
      "b_1: -53.530\n"
     ]
    }
   ],
   "source": [
    "# -- let's define the model\n",
    "# -- Look how to hidden_layer_sizes in the documentation\n",
    "params = {'hidden_layer_sizes': (1, ), \n",
    "          'solver' : 'lbfgs', \n",
    "          'random_state' : numero_di_matricola}\n",
    "train_model(X_train_scaled, Y_train, X_val_scaled, Y_val, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks vs Linear Models\n",
    "\n",
    "Let's learn a linear model on the same data and compare the results with the simple NN above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.26536\n",
      "Validation error: 0.31154\n",
      "\n",
      "--- Weights, with shape (18,) ---\n",
      "[-31303.71909156  35848.45081517  74506.78099995  -8012.41104949\n",
      "    671.23713588 100205.53195594  41671.19028923  19507.84532115\n",
      " 111331.50566184  69959.22677526  23468.73219785 -78236.93092911\n",
      "   6535.34729956 -28197.21476235  83701.76486765 -21647.26671149\n",
      "  22056.22833416  -2002.69401407]\n",
      "\n",
      "--- Bias --- \n",
      "536831.9203413766\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "LR = linear_model.LinearRegression()\n",
    "\n",
    "LR.fit(X_train_scaled, Y_train)\n",
    "\n",
    "# -- let's print the error (1 - R^2) on training data\n",
    "print(f'Training error: {(1.0 - LR.score(X_train_scaled, Y_train)):.5f}')\n",
    "# -- let's print the error (1 - R^2) on validation data\n",
    "print(f'Validation error: {(1.0 - LR.score(X_val_scaled, Y_val)):.5f}')\n",
    "\n",
    "print(f'\\n--- Weights, with shape {LR.coef_.shape} ---\\n{LR.coef_}')\n",
    "print(f'\\n--- Bias --- \\n{LR.intercept_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a way to make a NN network learn a linear model?\n",
    "\n",
    "Let's first check what is the activation function used by MLPRegressor..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.26536\n",
      "Validation error: 0.31154\n",
      "\n",
      "--- Weights of NN ---\n",
      "\n",
      "# Layer 1\n",
      "--- Weights, with shape (18, 1) ---\n",
      "w_(1, 1)^(1): 51.551\n",
      "w_(2, 1)^(1): -59.028\n",
      "w_(3, 1)^(1): -122.969\n",
      "w_(4, 1)^(1): 13.195\n",
      "w_(5, 1)^(1): -1.107\n",
      "w_(6, 1)^(1): -165.017\n",
      "w_(7, 1)^(1): -68.625\n",
      "w_(8, 1)^(1): -32.126\n",
      "w_(9, 1)^(1): -183.340\n",
      "w_(10, 1)^(1): -114.966\n",
      "w_(11, 1)^(1): -38.516\n",
      "w_(12, 1)^(1): 128.839\n",
      "w_(13, 1)^(1): -10.762\n",
      "w_(14, 1)^(1): 46.437\n",
      "w_(15, 1)^(1): -137.839\n",
      "w_(16, 1)^(1): 35.648\n",
      "w_(17, 1)^(1): -36.322\n",
      "w_(18, 1)^(1): 3.297\n",
      "--- Biases, with shape (1,) ---\n",
      "b_1: -883.447\n",
      "\n",
      "# Layer 2\n",
      "--- Weights, with shape (1, 1) ---\n",
      "w_(1, 1)^(2): -607.243\n",
      "--- Biases, with shape (1,) ---\n",
      "b_1: 365.292\n"
     ]
    }
   ],
   "source": [
    "# -- let's write the code to learn a linear model with NN: how? \n",
    "params = {'hidden_layer_sizes': (1, ), \n",
    "          'solver' : 'lbfgs', \n",
    "          'random_state' : numero_di_matricola, \n",
    "          'activation' : 'identity'\n",
    "         }\n",
    "train_model(X_train_scaled, Y_train, X_val_scaled, Y_val, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Example of handmade computations: with null input vector:\n",
    "# -- linear model output = bias ~ 536.831,9203\n",
    "# -- NN: w_(1, 1)^(2) * b_1 + b_2 ~ 536.829,396\n",
    "# -- why the above tiny difference? Because of l2 default regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is an $\\ell_2$ regularization term in MLPRegressor. What about making it smaller?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.26536\n",
      "Validation error: 0.31154\n",
      "\n",
      "--- Weights of NN ---\n",
      "\n",
      "# Layer 1\n",
      "--- Weights, with shape (18, 1) ---\n",
      "w_(1, 1)^(1): 51.551\n",
      "w_(2, 1)^(1): -59.028\n",
      "w_(3, 1)^(1): -122.969\n",
      "w_(4, 1)^(1): 13.195\n",
      "w_(5, 1)^(1): -1.107\n",
      "w_(6, 1)^(1): -165.017\n",
      "w_(7, 1)^(1): -68.625\n",
      "w_(8, 1)^(1): -32.126\n",
      "w_(9, 1)^(1): -183.340\n",
      "w_(10, 1)^(1): -114.966\n",
      "w_(11, 1)^(1): -38.516\n",
      "w_(12, 1)^(1): 128.839\n",
      "w_(13, 1)^(1): -10.762\n",
      "w_(14, 1)^(1): 46.437\n",
      "w_(15, 1)^(1): -137.839\n",
      "w_(16, 1)^(1): 35.648\n",
      "w_(17, 1)^(1): -36.322\n",
      "w_(18, 1)^(1): 3.297\n",
      "--- Biases, with shape (1,) ---\n",
      "b_1: -883.447\n",
      "\n",
      "# Layer 2\n",
      "--- Weights, with shape (1, 1) ---\n",
      "w_(1, 1)^(2): -607.243\n",
      "--- Biases, with shape (1,) ---\n",
      "b_1: 365.292\n"
     ]
    }
   ],
   "source": [
    "# -- you can try to change alpha (e.g., huge value to see the model is forcing null vector w)\n",
    "params = {'hidden_layer_sizes': (1, ), \n",
    "          'solver' : 'lbfgs', \n",
    "          'random_state' : numero_di_matricola, \n",
    "          'activation' : 'identity', \n",
    "          'alpha' : 1e-20\n",
    "         }\n",
    "train_model(X_train_scaled, Y_train, X_val_scaled, Y_val, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- with alpha = 1e-20: w_(1, 1)^(2) * b_1 + b_2 is 536.832,298621 (the difference is even closer, \n",
    "# -- not perfectly the same due to rounding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Complex NNs\n",
    "\n",
    "Let's try more complex NN, for example increasing the number of nodes in the only hidden layer, or increasing the number of hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a NN with 2 nodes in the only hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.18062\n",
      "Validation error: 0.20740\n",
      "\n",
      "--- Weights of NN ---\n",
      "\n",
      "# Layer 1\n",
      "--- Weights, with shape (18, 2) ---\n",
      "w_(1, 1)^(1): 91.025\n",
      "w_(1, 2)^(1): -33.276\n",
      "w_(2, 1)^(1): 120.426\n",
      "w_(2, 2)^(1): 39.341\n",
      "w_(3, 1)^(1): 85.919\n",
      "w_(3, 2)^(1): 72.986\n",
      "w_(4, 1)^(1): -271.772\n",
      "w_(4, 2)^(1): 28.298\n",
      "w_(5, 1)^(1): -30.709\n",
      "w_(5, 2)^(1): 17.828\n",
      "w_(6, 1)^(1): 197.574\n",
      "w_(6, 2)^(1): 25.890\n",
      "w_(7, 1)^(1): 34.917\n",
      "w_(7, 2)^(1): 37.556\n",
      "w_(8, 1)^(1): 96.981\n",
      "w_(8, 2)^(1): 25.686\n",
      "w_(9, 1)^(1): 312.804\n",
      "w_(9, 2)^(1): 132.752\n",
      "w_(10, 1)^(1): 85.171\n",
      "w_(10, 2)^(1): 68.792\n",
      "w_(11, 1)^(1): 19.298\n",
      "w_(11, 2)^(1): 23.331\n",
      "w_(12, 1)^(1): -217.580\n",
      "w_(12, 2)^(1): -81.109\n",
      "w_(13, 1)^(1): -3.451\n",
      "w_(13, 2)^(1): 20.168\n",
      "w_(14, 1)^(1): -300.967\n",
      "w_(14, 2)^(1): -26.398\n",
      "w_(15, 1)^(1): 305.200\n",
      "w_(15, 2)^(1): 144.630\n",
      "w_(16, 1)^(1): -463.259\n",
      "w_(16, 2)^(1): -16.429\n",
      "w_(17, 1)^(1): 193.671\n",
      "w_(17, 2)^(1): 53.022\n",
      "w_(18, 1)^(1): -241.365\n",
      "w_(18, 2)^(1): -11.182\n",
      "--- Biases, with shape (2,) ---\n",
      "b_1: -1049.808\n",
      "b_2: 897.576\n",
      "\n",
      "# Layer 2\n",
      "--- Weights, with shape (2, 1) ---\n",
      "w_(1, 1)^(2): 615.219\n",
      "w_(2, 1)^(2): 548.876\n",
      "--- Biases, with shape (1,) ---\n",
      "b_1: 725.967\n"
     ]
    }
   ],
   "source": [
    "# -- let's build a NN with 2 nodes in the only hidden layer\n",
    "params = {'hidden_layer_sizes': (2, ), 'solver' : 'lbfgs', 'random_state' : numero_di_matricola}\n",
    "train_model(X_train_scaled, Y_train, X_val_scaled, Y_val, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a NN with 5 nodes in the only hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.16289\n",
      "Validation error: 0.21691\n",
      "\n",
      "--- Weights of NN ---\n",
      "\n",
      "# Layer 1\n",
      "--- Weights, with shape (18, 5) ---\n",
      "w_(1, 1)^(1): -160.422\n",
      "w_(1, 2)^(1): 149.699\n",
      "w_(1, 3)^(1): 32.326\n",
      "w_(1, 4)^(1): 147.912\n",
      "w_(1, 5)^(1): -44.490\n",
      "w_(2, 1)^(1): 285.227\n",
      "w_(2, 2)^(1): 393.121\n",
      "w_(2, 3)^(1): 183.100\n",
      "w_(2, 4)^(1): 104.528\n",
      "w_(2, 5)^(1): 12.254\n",
      "w_(3, 1)^(1): -385.835\n",
      "w_(3, 2)^(1): 353.527\n",
      "w_(3, 3)^(1): 252.895\n",
      "w_(3, 4)^(1): -298.772\n",
      "w_(3, 5)^(1): 283.046\n",
      "w_(4, 1)^(1): -157.086\n",
      "w_(4, 2)^(1): -14.396\n",
      "w_(4, 3)^(1): -720.788\n",
      "w_(4, 4)^(1): 122.069\n",
      "w_(4, 5)^(1): 72.162\n",
      "w_(5, 1)^(1): 843.746\n",
      "w_(5, 2)^(1): -920.912\n",
      "w_(5, 3)^(1): 235.202\n",
      "w_(5, 4)^(1): -53.980\n",
      "w_(5, 5)^(1): -114.933\n",
      "w_(6, 1)^(1): -374.849\n",
      "w_(6, 2)^(1): 677.780\n",
      "w_(6, 3)^(1): 512.677\n",
      "w_(6, 4)^(1): -502.662\n",
      "w_(6, 5)^(1): -84.002\n",
      "w_(7, 1)^(1): -582.098\n",
      "w_(7, 2)^(1): 656.827\n",
      "w_(7, 3)^(1): -30.041\n",
      "w_(7, 4)^(1): -457.065\n",
      "w_(7, 5)^(1): 201.659\n",
      "w_(8, 1)^(1): 96.725\n",
      "w_(8, 2)^(1): 775.389\n",
      "w_(8, 3)^(1): 188.644\n",
      "w_(8, 4)^(1): -21.245\n",
      "w_(8, 5)^(1): 13.931\n",
      "w_(9, 1)^(1): -376.666\n",
      "w_(9, 2)^(1): 674.476\n",
      "w_(9, 3)^(1): 538.716\n",
      "w_(9, 4)^(1): 74.508\n",
      "w_(9, 5)^(1): 422.288\n",
      "w_(10, 1)^(1): -483.223\n",
      "w_(10, 2)^(1): 575.311\n",
      "w_(10, 3)^(1): 276.126\n",
      "w_(10, 4)^(1): -156.471\n",
      "w_(10, 5)^(1): 237.558\n",
      "w_(11, 1)^(1): 97.555\n",
      "w_(11, 2)^(1): -329.800\n",
      "w_(11, 3)^(1): 9.408\n",
      "w_(11, 4)^(1): -320.708\n",
      "w_(11, 5)^(1): 140.645\n",
      "w_(12, 1)^(1): 551.160\n",
      "w_(12, 2)^(1): -293.056\n",
      "w_(12, 3)^(1): -356.006\n",
      "w_(12, 4)^(1): -276.508\n",
      "w_(12, 5)^(1): -320.936\n",
      "w_(13, 1)^(1): 24.867\n",
      "w_(13, 2)^(1): 948.450\n",
      "w_(13, 3)^(1): -442.978\n",
      "w_(13, 4)^(1): -596.640\n",
      "w_(13, 5)^(1): 32.786\n",
      "w_(14, 1)^(1): 360.719\n",
      "w_(14, 2)^(1): -1306.096\n",
      "w_(14, 3)^(1): -536.321\n",
      "w_(14, 4)^(1): -186.391\n",
      "w_(14, 5)^(1): -102.106\n",
      "w_(15, 1)^(1): -443.004\n",
      "w_(15, 2)^(1): 424.247\n",
      "w_(15, 3)^(1): 784.049\n",
      "w_(15, 4)^(1): -13.312\n",
      "w_(15, 5)^(1): 475.140\n",
      "w_(16, 1)^(1): -126.914\n",
      "w_(16, 2)^(1): 827.079\n",
      "w_(16, 3)^(1): -1274.327\n",
      "w_(16, 4)^(1): 447.761\n",
      "w_(16, 5)^(1): -207.931\n",
      "w_(17, 1)^(1): 453.675\n",
      "w_(17, 2)^(1): 232.791\n",
      "w_(17, 3)^(1): 392.331\n",
      "w_(17, 4)^(1): 3.564\n",
      "w_(17, 5)^(1): 102.740\n",
      "w_(18, 1)^(1): 50.585\n",
      "w_(18, 2)^(1): -190.978\n",
      "w_(18, 3)^(1): -695.801\n",
      "w_(18, 4)^(1): 255.694\n",
      "w_(18, 5)^(1): -154.743\n",
      "--- Biases, with shape (5,) ---\n",
      "b_1: 1918.603\n",
      "b_2: -1064.738\n",
      "b_3: -2765.180\n",
      "b_4: 207.190\n",
      "b_5: 1384.142\n",
      "\n",
      "# Layer 2\n",
      "--- Weights, with shape (5, 1) ---\n",
      "w_(1, 1)^(2): 58.488\n",
      "w_(2, 1)^(2): 45.664\n",
      "w_(3, 1)^(2): 268.771\n",
      "w_(4, 1)^(2): 77.583\n",
      "w_(5, 1)^(2): 220.279\n",
      "--- Biases, with shape (1,) ---\n",
      "b_1: 397.093\n"
     ]
    }
   ],
   "source": [
    "# -- let's build a NN with 5 nodes in the only hidden layer\n",
    "params = {'hidden_layer_sizes': (5, ), 'solver' : 'lbfgs', 'random_state' : numero_di_matricola}\n",
    "train_model(X_train_scaled, Y_train, X_val_scaled, Y_val, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a NN with 10 nodes in the only hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.12100\n",
      "Validation error: 0.30936\n",
      "\n",
      "--- Weights of NN ---\n",
      "\n",
      "# Layer 1\n",
      "--- Weights, with shape (18, 10) ---\n",
      "w_(1, 1)^(1): 141.153\n",
      "w_(1, 2)^(1): -7.921\n",
      "w_(1, 3)^(1): -68.123\n",
      "w_(1, 4)^(1): 85.144\n",
      "w_(1, 5)^(1): 48.910\n",
      "w_(1, 6)^(1): -59.140\n",
      "w_(1, 7)^(1): 34.699\n",
      "w_(1, 8)^(1): -26.177\n",
      "w_(1, 9)^(1): -52.204\n",
      "w_(1, 10)^(1): 191.686\n",
      "w_(2, 1)^(1): 15.847\n",
      "w_(2, 2)^(1): -62.052\n",
      "w_(2, 3)^(1): 101.619\n",
      "w_(2, 4)^(1): 53.488\n",
      "w_(2, 5)^(1): -92.146\n",
      "w_(2, 6)^(1): 409.741\n",
      "w_(2, 7)^(1): 290.965\n",
      "w_(2, 8)^(1): -2.751\n",
      "w_(2, 9)^(1): 114.179\n",
      "w_(2, 10)^(1): -156.482\n",
      "w_(3, 1)^(1): -129.727\n",
      "w_(3, 2)^(1): 35.891\n",
      "w_(3, 3)^(1): 246.597\n",
      "w_(3, 4)^(1): 16.031\n",
      "w_(3, 5)^(1): -178.390\n",
      "w_(3, 6)^(1): 175.217\n",
      "w_(3, 7)^(1): -197.011\n",
      "w_(3, 8)^(1): -7.467\n",
      "w_(3, 9)^(1): 111.412\n",
      "w_(3, 10)^(1): -166.436\n",
      "w_(4, 1)^(1): 191.858\n",
      "w_(4, 2)^(1): 39.122\n",
      "w_(4, 3)^(1): 70.708\n",
      "w_(4, 4)^(1): -190.881\n",
      "w_(4, 5)^(1): -97.799\n",
      "w_(4, 6)^(1): -11.803\n",
      "w_(4, 7)^(1): -207.882\n",
      "w_(4, 8)^(1): -124.910\n",
      "w_(4, 9)^(1): -38.369\n",
      "w_(4, 10)^(1): -502.578\n",
      "w_(5, 1)^(1): -535.652\n",
      "w_(5, 2)^(1): 561.129\n",
      "w_(5, 3)^(1): -182.642\n",
      "w_(5, 4)^(1): 71.903\n",
      "w_(5, 5)^(1): 193.379\n",
      "w_(5, 6)^(1): -22.476\n",
      "w_(5, 7)^(1): -179.707\n",
      "w_(5, 8)^(1): 201.221\n",
      "w_(5, 9)^(1): 181.013\n",
      "w_(5, 10)^(1): -584.348\n",
      "w_(6, 1)^(1): 278.707\n",
      "w_(6, 2)^(1): -200.743\n",
      "w_(6, 3)^(1): -123.634\n",
      "w_(6, 4)^(1): 107.890\n",
      "w_(6, 5)^(1): -350.440\n",
      "w_(6, 6)^(1): 131.702\n",
      "w_(6, 7)^(1): -223.251\n",
      "w_(6, 8)^(1): -81.027\n",
      "w_(6, 9)^(1): 219.934\n",
      "w_(6, 10)^(1): 461.177\n",
      "w_(7, 1)^(1): -24.347\n",
      "w_(7, 2)^(1): -185.685\n",
      "w_(7, 3)^(1): 17.350\n",
      "w_(7, 4)^(1): 123.529\n",
      "w_(7, 5)^(1): 144.378\n",
      "w_(7, 6)^(1): -162.374\n",
      "w_(7, 7)^(1): 140.397\n",
      "w_(7, 8)^(1): -69.902\n",
      "w_(7, 9)^(1): -75.469\n",
      "w_(7, 10)^(1): 23.327\n",
      "w_(8, 1)^(1): -92.972\n",
      "w_(8, 2)^(1): -302.655\n",
      "w_(8, 3)^(1): -64.677\n",
      "w_(8, 4)^(1): 83.024\n",
      "w_(8, 5)^(1): 351.960\n",
      "w_(8, 6)^(1): 349.816\n",
      "w_(8, 7)^(1): 6.796\n",
      "w_(8, 8)^(1): 92.817\n",
      "w_(8, 9)^(1): 112.148\n",
      "w_(8, 10)^(1): -115.879\n",
      "w_(9, 1)^(1): -104.905\n",
      "w_(9, 2)^(1): -122.246\n",
      "w_(9, 3)^(1): 214.641\n",
      "w_(9, 4)^(1): 366.611\n",
      "w_(9, 5)^(1): 109.301\n",
      "w_(9, 6)^(1): 277.124\n",
      "w_(9, 7)^(1): 196.784\n",
      "w_(9, 8)^(1): -21.411\n",
      "w_(9, 9)^(1): 135.027\n",
      "w_(9, 10)^(1): 439.314\n",
      "w_(10, 1)^(1): -23.893\n",
      "w_(10, 2)^(1): -7.545\n",
      "w_(10, 3)^(1): 229.383\n",
      "w_(10, 4)^(1): 64.303\n",
      "w_(10, 5)^(1): -162.119\n",
      "w_(10, 6)^(1): 130.074\n",
      "w_(10, 7)^(1): -38.731\n",
      "w_(10, 8)^(1): -22.788\n",
      "w_(10, 9)^(1): 12.545\n",
      "w_(10, 10)^(1): -185.013\n",
      "w_(11, 1)^(1): -219.955\n",
      "w_(11, 2)^(1): 86.071\n",
      "w_(11, 3)^(1): 82.223\n",
      "w_(11, 4)^(1): -84.704\n",
      "w_(11, 5)^(1): -66.442\n",
      "w_(11, 6)^(1): 116.652\n",
      "w_(11, 7)^(1): -326.514\n",
      "w_(11, 8)^(1): 27.667\n",
      "w_(11, 9)^(1): 202.158\n",
      "w_(11, 10)^(1): -0.318\n",
      "w_(12, 1)^(1): 285.458\n",
      "w_(12, 2)^(1): 171.453\n",
      "w_(12, 3)^(1): 146.929\n",
      "w_(12, 4)^(1): -307.884\n",
      "w_(12, 5)^(1): -411.437\n",
      "w_(12, 6)^(1): 20.867\n",
      "w_(12, 7)^(1): 137.900\n",
      "w_(12, 8)^(1): -172.225\n",
      "w_(12, 9)^(1): 166.282\n",
      "w_(12, 10)^(1): -923.861\n",
      "w_(13, 1)^(1): -90.075\n",
      "w_(13, 2)^(1): -529.168\n",
      "w_(13, 3)^(1): 161.405\n",
      "w_(13, 4)^(1): 80.133\n",
      "w_(13, 5)^(1): -41.458\n",
      "w_(13, 6)^(1): 380.783\n",
      "w_(13, 7)^(1): 79.227\n",
      "w_(13, 8)^(1): -165.519\n",
      "w_(13, 9)^(1): -669.340\n",
      "w_(13, 10)^(1): -357.843\n",
      "w_(14, 1)^(1): -195.969\n",
      "w_(14, 2)^(1): 272.744\n",
      "w_(14, 3)^(1): -50.241\n",
      "w_(14, 4)^(1): -582.162\n",
      "w_(14, 5)^(1): -136.911\n",
      "w_(14, 6)^(1): -215.885\n",
      "w_(14, 7)^(1): -55.972\n",
      "w_(14, 8)^(1): -26.501\n",
      "w_(14, 9)^(1): 75.943\n",
      "w_(14, 10)^(1): 67.924\n",
      "w_(15, 1)^(1): 18.813\n",
      "w_(15, 2)^(1): -123.372\n",
      "w_(15, 3)^(1): 253.287\n",
      "w_(15, 4)^(1): 280.499\n",
      "w_(15, 5)^(1): 56.806\n",
      "w_(15, 6)^(1): 334.383\n",
      "w_(15, 7)^(1): 151.906\n",
      "w_(15, 8)^(1): 26.568\n",
      "w_(15, 9)^(1): 337.229\n",
      "w_(15, 10)^(1): 113.126\n",
      "w_(16, 1)^(1): 584.930\n",
      "w_(16, 2)^(1): -128.247\n",
      "w_(16, 3)^(1): -117.915\n",
      "w_(16, 4)^(1): -277.465\n",
      "w_(16, 5)^(1): -23.602\n",
      "w_(16, 6)^(1): 375.658\n",
      "w_(16, 7)^(1): -449.626\n",
      "w_(16, 8)^(1): -179.214\n",
      "w_(16, 9)^(1): -332.294\n",
      "w_(16, 10)^(1): -265.927\n",
      "w_(17, 1)^(1): 179.767\n",
      "w_(17, 2)^(1): -350.705\n",
      "w_(17, 3)^(1): -112.606\n",
      "w_(17, 4)^(1): 60.773\n",
      "w_(17, 5)^(1): 442.391\n",
      "w_(17, 6)^(1): 130.221\n",
      "w_(17, 7)^(1): 249.596\n",
      "w_(17, 8)^(1): 287.025\n",
      "w_(17, 9)^(1): 44.126\n",
      "w_(17, 10)^(1): 609.949\n",
      "w_(18, 1)^(1): -259.436\n",
      "w_(18, 2)^(1): 178.386\n",
      "w_(18, 3)^(1): -37.110\n",
      "w_(18, 4)^(1): 103.039\n",
      "w_(18, 5)^(1): 11.926\n",
      "w_(18, 6)^(1): -132.127\n",
      "w_(18, 7)^(1): -209.226\n",
      "w_(18, 8)^(1): -190.919\n",
      "w_(18, 9)^(1): -82.882\n",
      "w_(18, 10)^(1): -343.639\n",
      "--- Biases, with shape (10,) ---\n",
      "b_1: -202.566\n",
      "b_2: 621.941\n",
      "b_3: 880.148\n",
      "b_4: -1074.647\n",
      "b_5: 805.051\n",
      "b_6: 411.629\n",
      "b_7: -1203.725\n",
      "b_8: -518.045\n",
      "b_9: -1506.936\n",
      "b_10: -547.500\n",
      "\n",
      "# Layer 2\n",
      "--- Weights, with shape (10, 1) ---\n",
      "w_(1, 1)^(2): 81.918\n",
      "w_(2, 1)^(2): 73.257\n",
      "w_(3, 1)^(2): 252.936\n",
      "w_(4, 1)^(2): 391.619\n",
      "w_(5, 1)^(2): 136.370\n",
      "w_(6, 1)^(2): 47.818\n",
      "w_(7, 1)^(2): 1197.232\n",
      "w_(8, 1)^(2): 460.898\n",
      "w_(9, 1)^(2): 723.731\n",
      "w_(10, 1)^(2): 84.134\n",
      "--- Biases, with shape (1,) ---\n",
      "b_1: 364.462\n"
     ]
    }
   ],
   "source": [
    "# -- let's build a NN with 10 nodes in the only hidden layer\n",
    "params = {'hidden_layer_sizes': (10, ), 'solver' : 'lbfgs', 'random_state' : numero_di_matricola}\n",
    "train_model(X_train_scaled, Y_train, X_val_scaled, Y_val, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a NN with 100 nodes in the only hidden layer. Note that this is the default!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.03129\n",
      "Validation error: 0.45291\n"
     ]
    }
   ],
   "source": [
    "# -- let's build a NN with 100 nodes in the only hidden layer\n",
    "params = {'hidden_layer_sizes': (100, ), 'solver' : 'lbfgs', 'random_state' : numero_di_matricola}\n",
    "train_model(X_train_scaled, Y_train, X_val_scaled, Y_val, print_weights=False, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try 2 layers, 1 node each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.23949\n",
      "Validation error: 0.27328\n",
      "\n",
      "--- Weights of NN ---\n",
      "\n",
      "# Layer 1\n",
      "--- Weights, with shape (18, 1) ---\n",
      "w_(1, 1)^(1): -30.571\n",
      "w_(2, 1)^(1): 86.994\n",
      "w_(3, 1)^(1): 145.031\n",
      "w_(4, 1)^(1): -55.230\n",
      "w_(5, 1)^(1): 5.310\n",
      "w_(6, 1)^(1): 185.607\n",
      "w_(7, 1)^(1): 67.663\n",
      "w_(8, 1)^(1): 62.209\n",
      "w_(9, 1)^(1): 287.436\n",
      "w_(10, 1)^(1): 139.335\n",
      "w_(11, 1)^(1): 41.077\n",
      "w_(12, 1)^(1): -174.304\n",
      "w_(13, 1)^(1): 10.097\n",
      "w_(14, 1)^(1): -129.047\n",
      "w_(15, 1)^(1): 266.308\n",
      "w_(16, 1)^(1): -196.268\n",
      "w_(17, 1)^(1): 100.454\n",
      "w_(18, 1)^(1): -53.822\n",
      "--- Biases, with shape (1,) ---\n",
      "b_1: 151.965\n",
      "\n",
      "# Layer 2\n",
      "--- Weights, with shape (1, 1) ---\n",
      "w_(1, 1)^(2): 1.271\n",
      "--- Biases, with shape (1,) ---\n",
      "b_1: 746.543\n",
      "\n",
      "# Layer 3\n",
      "--- Weights, with shape (1, 1) ---\n",
      "w_(1, 1)^(3): 439.488\n",
      "--- Biases, with shape (1,) ---\n",
      "b_1: 471.667\n"
     ]
    }
   ],
   "source": [
    "# -- let's build a NN with 2 hidden layers each with a node\n",
    "params = {'hidden_layer_sizes': (1, 1), 'solver' : 'lbfgs', 'random_state' : numero_di_matricola}\n",
    "train_model(X_train_scaled, Y_train, X_val_scaled, Y_val, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try 2 layers, 2 nodes each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.21263\n",
      "Validation error: 0.26926\n",
      "\n",
      "--- Weights of NN ---\n",
      "\n",
      "# Layer 1\n",
      "--- Weights, with shape (18, 2) ---\n",
      "w_(1, 1)^(1): -3.656\n",
      "w_(1, 2)^(1): 0.584\n",
      "w_(2, 1)^(1): 10.093\n",
      "w_(2, 2)^(1): -5.125\n",
      "w_(3, 1)^(1): 20.595\n",
      "w_(3, 2)^(1): -21.713\n",
      "w_(4, 1)^(1): -7.868\n",
      "w_(4, 2)^(1): 10.587\n",
      "w_(5, 1)^(1): -4.502\n",
      "w_(5, 2)^(1): 12.553\n",
      "w_(6, 1)^(1): 20.980\n",
      "w_(6, 2)^(1): -21.485\n",
      "w_(7, 1)^(1): 9.432\n",
      "w_(7, 2)^(1): -9.771\n",
      "w_(8, 1)^(1): 6.813\n",
      "w_(8, 2)^(1): -4.419\n",
      "w_(9, 1)^(1): 37.122\n",
      "w_(9, 2)^(1): -32.245\n",
      "w_(10, 1)^(1): 20.181\n",
      "w_(10, 2)^(1): -21.483\n",
      "w_(11, 1)^(1): 5.354\n",
      "w_(11, 2)^(1): -4.038\n",
      "w_(12, 1)^(1): -27.572\n",
      "w_(12, 2)^(1): 29.283\n",
      "w_(13, 1)^(1): 0.668\n",
      "w_(13, 2)^(1): 2.305\n",
      "w_(14, 1)^(1): -15.230\n",
      "w_(14, 2)^(1): 16.215\n",
      "w_(15, 1)^(1): 24.875\n",
      "w_(15, 2)^(1): -7.892\n",
      "w_(16, 1)^(1): -17.959\n",
      "w_(16, 2)^(1): 20.163\n",
      "w_(17, 1)^(1): 5.433\n",
      "w_(17, 2)^(1): 5.135\n",
      "w_(18, 1)^(1): -3.163\n",
      "w_(18, 2)^(1): 5.838\n",
      "--- Biases, with shape (2,) ---\n",
      "b_1: 80.038\n",
      "b_2: 56.388\n",
      "\n",
      "# Layer 2\n",
      "--- Weights, with shape (2, 2) ---\n",
      "w_(1, 1)^(2): -3.829\n",
      "w_(1, 2)^(2): 70.123\n",
      "w_(2, 1)^(2): -23.150\n",
      "w_(2, 2)^(2): 33.920\n",
      "--- Biases, with shape (2,) ---\n",
      "b_1: -17.565\n",
      "b_2: -506.224\n",
      "\n",
      "# Layer 3\n",
      "--- Weights, with shape (2, 1) ---\n",
      "w_(1, 1)^(3): 9.724\n",
      "w_(2, 1)^(3): 69.666\n",
      "--- Biases, with shape (1,) ---\n",
      "b_1: 158.855\n"
     ]
    }
   ],
   "source": [
    "# -- let's build a NN with 2 hidden layers each with two nodes\n",
    "params = {'hidden_layer_sizes': (2, 2), 'solver' : 'lbfgs', 'random_state' : numero_di_matricola}\n",
    "train_model(X_train_scaled, Y_train, X_val_scaled, Y_val, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try other architectures! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.06621\n",
      "Validation error: 0.29563\n",
      "\n",
      "--- Weights of NN ---\n",
      "\n",
      "# Layer 1\n",
      "--- Weights, with shape (18, 10) ---\n",
      "w_(1, 1)^(1): 8.934\n",
      "w_(1, 2)^(1): 8.516\n",
      "w_(1, 3)^(1): 9.824\n",
      "w_(1, 4)^(1): 0.181\n",
      "w_(1, 5)^(1): -13.816\n",
      "w_(1, 6)^(1): -29.702\n",
      "w_(1, 7)^(1): -7.791\n",
      "w_(1, 8)^(1): -19.290\n",
      "w_(1, 9)^(1): -7.714\n",
      "w_(1, 10)^(1): 17.025\n",
      "w_(2, 1)^(1): 1.459\n",
      "w_(2, 2)^(1): 32.388\n",
      "w_(2, 3)^(1): 24.639\n",
      "w_(2, 4)^(1): 16.192\n",
      "w_(2, 5)^(1): -15.690\n",
      "w_(2, 6)^(1): 47.378\n",
      "w_(2, 7)^(1): -7.874\n",
      "w_(2, 8)^(1): 7.860\n",
      "w_(2, 9)^(1): -18.916\n",
      "w_(2, 10)^(1): -10.852\n",
      "w_(3, 1)^(1): -26.572\n",
      "w_(3, 2)^(1): 19.352\n",
      "w_(3, 3)^(1): 12.468\n",
      "w_(3, 4)^(1): -8.192\n",
      "w_(3, 5)^(1): -10.840\n",
      "w_(3, 6)^(1): 11.789\n",
      "w_(3, 7)^(1): 7.506\n",
      "w_(3, 8)^(1): 34.334\n",
      "w_(3, 9)^(1): -40.321\n",
      "w_(3, 10)^(1): -4.954\n",
      "w_(4, 1)^(1): -6.157\n",
      "w_(4, 2)^(1): -0.739\n",
      "w_(4, 3)^(1): 13.458\n",
      "w_(4, 4)^(1): -20.574\n",
      "w_(4, 5)^(1): -22.031\n",
      "w_(4, 6)^(1): -5.736\n",
      "w_(4, 7)^(1): -7.748\n",
      "w_(4, 8)^(1): -6.227\n",
      "w_(4, 9)^(1): 5.488\n",
      "w_(4, 10)^(1): -0.545\n",
      "w_(5, 1)^(1): -33.640\n",
      "w_(5, 2)^(1): 26.048\n",
      "w_(5, 3)^(1): -20.782\n",
      "w_(5, 4)^(1): -9.310\n",
      "w_(5, 5)^(1): 19.361\n",
      "w_(5, 6)^(1): -57.238\n",
      "w_(5, 7)^(1): 10.926\n",
      "w_(5, 8)^(1): -35.705\n",
      "w_(5, 9)^(1): 16.947\n",
      "w_(5, 10)^(1): -80.790\n",
      "w_(6, 1)^(1): -32.381\n",
      "w_(6, 2)^(1): 13.051\n",
      "w_(6, 3)^(1): 10.518\n",
      "w_(6, 4)^(1): -42.487\n",
      "w_(6, 5)^(1): -47.083\n",
      "w_(6, 6)^(1): 35.795\n",
      "w_(6, 7)^(1): -26.517\n",
      "w_(6, 8)^(1): -14.488\n",
      "w_(6, 9)^(1): -61.685\n",
      "w_(6, 10)^(1): 36.132\n",
      "w_(7, 1)^(1): -23.477\n",
      "w_(7, 2)^(1): 42.751\n",
      "w_(7, 3)^(1): 22.718\n",
      "w_(7, 4)^(1): -21.456\n",
      "w_(7, 5)^(1): -6.901\n",
      "w_(7, 6)^(1): -18.167\n",
      "w_(7, 7)^(1): -30.999\n",
      "w_(7, 8)^(1): 56.718\n",
      "w_(7, 9)^(1): -15.397\n",
      "w_(7, 10)^(1): -6.510\n",
      "w_(8, 1)^(1): -10.641\n",
      "w_(8, 2)^(1): 22.033\n",
      "w_(8, 3)^(1): 14.158\n",
      "w_(8, 4)^(1): -8.981\n",
      "w_(8, 5)^(1): 31.091\n",
      "w_(8, 6)^(1): -72.383\n",
      "w_(8, 7)^(1): -13.559\n",
      "w_(8, 8)^(1): 66.445\n",
      "w_(8, 9)^(1): 20.040\n",
      "w_(8, 10)^(1): 0.420\n",
      "w_(9, 1)^(1): 0.046\n",
      "w_(9, 2)^(1): 37.735\n",
      "w_(9, 3)^(1): 45.986\n",
      "w_(9, 4)^(1): -0.470\n",
      "w_(9, 5)^(1): 10.328\n",
      "w_(9, 6)^(1): 34.931\n",
      "w_(9, 7)^(1): -18.251\n",
      "w_(9, 8)^(1): 40.629\n",
      "w_(9, 9)^(1): -36.912\n",
      "w_(9, 10)^(1): 30.213\n",
      "w_(10, 1)^(1): -9.107\n",
      "w_(10, 2)^(1): 23.693\n",
      "w_(10, 3)^(1): 19.631\n",
      "w_(10, 4)^(1): -7.469\n",
      "w_(10, 5)^(1): -12.612\n",
      "w_(10, 6)^(1): 17.223\n",
      "w_(10, 7)^(1): 2.058\n",
      "w_(10, 8)^(1): 42.722\n",
      "w_(10, 9)^(1): -45.293\n",
      "w_(10, 10)^(1): -9.267\n",
      "w_(11, 1)^(1): -38.496\n",
      "w_(11, 2)^(1): -4.023\n",
      "w_(11, 3)^(1): -9.832\n",
      "w_(11, 4)^(1): -3.343\n",
      "w_(11, 5)^(1): 0.524\n",
      "w_(11, 6)^(1): -8.523\n",
      "w_(11, 7)^(1): 12.426\n",
      "w_(11, 8)^(1): -6.830\n",
      "w_(11, 9)^(1): 0.749\n",
      "w_(11, 10)^(1): 6.767\n",
      "w_(12, 1)^(1): -19.153\n",
      "w_(12, 2)^(1): 29.164\n",
      "w_(12, 3)^(1): 3.171\n",
      "w_(12, 4)^(1): -7.661\n",
      "w_(12, 5)^(1): -93.065\n",
      "w_(12, 6)^(1): 119.327\n",
      "w_(12, 7)^(1): 10.203\n",
      "w_(12, 8)^(1): -16.113\n",
      "w_(12, 9)^(1): -42.376\n",
      "w_(12, 10)^(1): -132.910\n",
      "w_(13, 1)^(1): -13.359\n",
      "w_(13, 2)^(1): -36.395\n",
      "w_(13, 3)^(1): -3.147\n",
      "w_(13, 4)^(1): -14.037\n",
      "w_(13, 5)^(1): 95.873\n",
      "w_(13, 6)^(1): 17.664\n",
      "w_(13, 7)^(1): -77.140\n",
      "w_(13, 8)^(1): -35.380\n",
      "w_(13, 9)^(1): 34.440\n",
      "w_(13, 10)^(1): -88.479\n",
      "w_(14, 1)^(1): -71.654\n",
      "w_(14, 2)^(1): -115.352\n",
      "w_(14, 3)^(1): -8.040\n",
      "w_(14, 4)^(1): 12.194\n",
      "w_(14, 5)^(1): 21.252\n",
      "w_(14, 6)^(1): 29.033\n",
      "w_(14, 7)^(1): 6.390\n",
      "w_(14, 8)^(1): -20.755\n",
      "w_(14, 9)^(1): 6.793\n",
      "w_(14, 10)^(1): -32.206\n",
      "w_(15, 1)^(1): -33.151\n",
      "w_(15, 2)^(1): 33.105\n",
      "w_(15, 3)^(1): 36.382\n",
      "w_(15, 4)^(1): -195.417\n",
      "w_(15, 5)^(1): 44.370\n",
      "w_(15, 6)^(1): -5.814\n",
      "w_(15, 7)^(1): -102.340\n",
      "w_(15, 8)^(1): 55.269\n",
      "w_(15, 9)^(1): 95.946\n",
      "w_(15, 10)^(1): 76.346\n",
      "w_(16, 1)^(1): 14.700\n",
      "w_(16, 2)^(1): -65.295\n",
      "w_(16, 3)^(1): 22.441\n",
      "w_(16, 4)^(1): 12.373\n",
      "w_(16, 5)^(1): -56.411\n",
      "w_(16, 6)^(1): -10.034\n",
      "w_(16, 7)^(1): -26.390\n",
      "w_(16, 8)^(1): -2.689\n",
      "w_(16, 9)^(1): -18.470\n",
      "w_(16, 10)^(1): -11.448\n",
      "w_(17, 1)^(1): 15.537\n",
      "w_(17, 2)^(1): -1.795\n",
      "w_(17, 3)^(1): -9.744\n",
      "w_(17, 4)^(1): -10.705\n",
      "w_(17, 5)^(1): 61.401\n",
      "w_(17, 6)^(1): -43.123\n",
      "w_(17, 7)^(1): 1.269\n",
      "w_(17, 8)^(1): 45.498\n",
      "w_(17, 9)^(1): 13.976\n",
      "w_(17, 10)^(1): 48.645\n",
      "w_(18, 1)^(1): -8.098\n",
      "w_(18, 2)^(1): 3.366\n",
      "w_(18, 3)^(1): -9.482\n",
      "w_(18, 4)^(1): -4.745\n",
      "w_(18, 5)^(1): 1.537\n",
      "w_(18, 6)^(1): -39.415\n",
      "w_(18, 7)^(1): 2.914\n",
      "w_(18, 8)^(1): 2.486\n",
      "w_(18, 9)^(1): -4.884\n",
      "w_(18, 10)^(1): -26.093\n",
      "--- Biases, with shape (10,) ---\n",
      "b_1: 81.037\n",
      "b_2: -214.611\n",
      "b_3: 159.296\n",
      "b_4: 70.336\n",
      "b_5: 37.332\n",
      "b_6: -69.041\n",
      "b_7: 127.677\n",
      "b_8: -27.658\n",
      "b_9: 1.203\n",
      "b_10: -86.428\n",
      "\n",
      "# Layer 2\n",
      "--- Weights, with shape (10, 10) ---\n",
      "w_(1, 1)^(2): -0.830\n",
      "w_(1, 2)^(2): -0.439\n",
      "w_(1, 3)^(2): 16.563\n",
      "w_(1, 4)^(2): -3.953\n",
      "w_(1, 5)^(2): -9.283\n",
      "w_(1, 6)^(2): -0.553\n",
      "w_(1, 7)^(2): -0.538\n",
      "w_(1, 8)^(2): -1.746\n",
      "w_(1, 9)^(2): 21.367\n",
      "w_(1, 10)^(2): -5.539\n",
      "w_(2, 1)^(2): 13.549\n",
      "w_(2, 2)^(2): 10.643\n",
      "w_(2, 3)^(2): -2.434\n",
      "w_(2, 4)^(2): 5.063\n",
      "w_(2, 5)^(2): -2.809\n",
      "w_(2, 6)^(2): -1.375\n",
      "w_(2, 7)^(2): -2.222\n",
      "w_(2, 8)^(2): -10.999\n",
      "w_(2, 9)^(2): 44.371\n",
      "w_(2, 10)^(2): -8.890\n",
      "w_(3, 1)^(2): 22.081\n",
      "w_(3, 2)^(2): 66.860\n",
      "w_(3, 3)^(2): 14.562\n",
      "w_(3, 4)^(2): 2.029\n",
      "w_(3, 5)^(2): -4.580\n",
      "w_(3, 6)^(2): -0.794\n",
      "w_(3, 7)^(2): -0.564\n",
      "w_(3, 8)^(2): -6.087\n",
      "w_(3, 9)^(2): -35.035\n",
      "w_(3, 10)^(2): -4.976\n",
      "w_(4, 1)^(2): -13.666\n",
      "w_(4, 2)^(2): -10.414\n",
      "w_(4, 3)^(2): -52.857\n",
      "w_(4, 4)^(2): -34.977\n",
      "w_(4, 5)^(2): -1.994\n",
      "w_(4, 6)^(2): -0.154\n",
      "w_(4, 7)^(2): -0.463\n",
      "w_(4, 8)^(2): -4.196\n",
      "w_(4, 9)^(2): -111.464\n",
      "w_(4, 10)^(2): -3.564\n",
      "w_(5, 1)^(2): 8.163\n",
      "w_(5, 2)^(2): 34.705\n",
      "w_(5, 3)^(2): -2.402\n",
      "w_(5, 4)^(2): 11.297\n",
      "w_(5, 5)^(2): -7.171\n",
      "w_(5, 6)^(2): -0.617\n",
      "w_(5, 7)^(2): -0.534\n",
      "w_(5, 8)^(2): -2.568\n",
      "w_(5, 9)^(2): 27.913\n",
      "w_(5, 10)^(2): -4.404\n",
      "w_(6, 1)^(2): 0.986\n",
      "w_(6, 2)^(2): 1.710\n",
      "w_(6, 3)^(2): -8.540\n",
      "w_(6, 4)^(2): 4.442\n",
      "w_(6, 5)^(2): -4.919\n",
      "w_(6, 6)^(2): -0.611\n",
      "w_(6, 7)^(2): -0.609\n",
      "w_(6, 8)^(2): -5.816\n",
      "w_(6, 9)^(2): 29.587\n",
      "w_(6, 10)^(2): -4.869\n",
      "w_(7, 1)^(2): 16.811\n",
      "w_(7, 2)^(2): 38.636\n",
      "w_(7, 3)^(2): 41.326\n",
      "w_(7, 4)^(2): 22.352\n",
      "w_(7, 5)^(2): -3.705\n",
      "w_(7, 6)^(2): -0.147\n",
      "w_(7, 7)^(2): -0.066\n",
      "w_(7, 8)^(2): -1.896\n",
      "w_(7, 9)^(2): 23.039\n",
      "w_(7, 10)^(2): -2.660\n",
      "w_(8, 1)^(2): 6.529\n",
      "w_(8, 2)^(2): 10.830\n",
      "w_(8, 3)^(2): -1.842\n",
      "w_(8, 4)^(2): 6.151\n",
      "w_(8, 5)^(2): -5.394\n",
      "w_(8, 6)^(2): -0.671\n",
      "w_(8, 7)^(2): -0.145\n",
      "w_(8, 8)^(2): -3.691\n",
      "w_(8, 9)^(2): 0.723\n",
      "w_(8, 10)^(2): -4.162\n",
      "w_(9, 1)^(2): -6.207\n",
      "w_(9, 2)^(2): -12.467\n",
      "w_(9, 3)^(2): 3.785\n",
      "w_(9, 4)^(2): -17.220\n",
      "w_(9, 5)^(2): -7.187\n",
      "w_(9, 6)^(2): -0.593\n",
      "w_(9, 7)^(2): -0.369\n",
      "w_(9, 8)^(2): -3.778\n",
      "w_(9, 9)^(2): -89.044\n",
      "w_(9, 10)^(2): -4.306\n",
      "w_(10, 1)^(2): 1.265\n",
      "w_(10, 2)^(2): -10.678\n",
      "w_(10, 3)^(2): -4.791\n",
      "w_(10, 4)^(2): 13.492\n",
      "w_(10, 5)^(2): -6.931\n",
      "w_(10, 6)^(2): -1.185\n",
      "w_(10, 7)^(2): -1.104\n",
      "w_(10, 8)^(2): -10.443\n",
      "w_(10, 9)^(2): 23.271\n",
      "w_(10, 10)^(2): -8.746\n",
      "--- Biases, with shape (10,) ---\n",
      "b_1: 10.712\n",
      "b_2: 20.021\n",
      "b_3: 10.504\n",
      "b_4: 1.773\n",
      "b_5: -12.630\n",
      "b_6: -1.933\n",
      "b_7: -0.896\n",
      "b_8: -7.914\n",
      "b_9: 3.874\n",
      "b_10: -10.535\n",
      "\n",
      "# Layer 3\n",
      "--- Weights, with shape (10, 1) ---\n",
      "w_(1, 1)^(3): 2.011\n",
      "w_(2, 1)^(3): 19.195\n",
      "w_(3, 1)^(3): 30.332\n",
      "w_(4, 1)^(3): 27.224\n",
      "w_(5, 1)^(3): 12.307\n",
      "w_(6, 1)^(3): 22.642\n",
      "w_(7, 1)^(3): 0.762\n",
      "w_(8, 1)^(3): 11.947\n",
      "w_(9, 1)^(3): 119.608\n",
      "w_(10, 1)^(3): 38.794\n",
      "--- Biases, with shape (1,) ---\n",
      "b_1: 42.514\n"
     ]
    }
   ],
   "source": [
    "# -- let's build a NN with 2 hidden layers each with 10 nodes\n",
    "params = {'hidden_layer_sizes': (10, 10), 'solver' : 'lbfgs', 'random_state' : numero_di_matricola}\n",
    "train_model(X_train_scaled, Y_train, X_val_scaled, Y_val, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.02315\n",
      "Validation error: 0.33635\n"
     ]
    }
   ],
   "source": [
    "# -- let's build a NN with 2 hidden layers each with 100 nodes\n",
    "params = {'hidden_layer_sizes': (100, 100), 'solver' : 'lbfgs', 'random_state' : numero_di_matricola}\n",
    "train_model(X_train_scaled, Y_train, X_val_scaled, Y_val, print_weights=False, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we find the best architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try 5-fold cross-validation with number of nodes in the hidden layer between 1 and 20. Note that we use train and validation data together, since we are doing cross-validation.\n",
    "\n",
    "Note: you can also try to change the maximum amount of iterations to see what happens (see documentation for max_iter parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def k_fold_cross_validation(X_train, Y_train, random_state, num_folds = 5):\n",
    "\n",
    "    # -- grid of hyperparams\n",
    "    param_grid = {'hidden_layer_sizes': [i for i in range(1, 21)],\n",
    "                  'activation': ['relu'],\n",
    "                  'solver': ['lbfgs'],\n",
    "                  'random_state': [random_state],\n",
    "                  'max_iter': [150, 175, 200]\n",
    "                 }\n",
    "\n",
    "    param_list = [\n",
    "    {'hidden_layer_sizes': hls, 'activation': act, 'solver': solv, 'random_state': rs, 'max_iter': mit}\n",
    "    for hls, act, solv, rs, mit in product(\n",
    "        param_grid['hidden_layer_sizes'],\n",
    "        param_grid['activation'],\n",
    "        param_grid['solver'],\n",
    "        param_grid['random_state'],\n",
    "        param_grid['max_iter']\n",
    "    )\n",
    "    ]\n",
    "    \n",
    "    err_train_kfold = np.zeros(len(param_list),)\n",
    "    err_val_kfold = np.zeros(len(param_list),)\n",
    "    \n",
    "    # print('Params for model selection:', param_list)\n",
    "\n",
    "    kf = KFold(n_splits = num_folds)\n",
    "\n",
    "\n",
    "    # -- perform kfold validation for model selection (k = 5)\n",
    "    for i, params in enumerate(param_list):\n",
    "    \n",
    "        print(f'#{i+1} Performing k-fold for params = {params}...')\n",
    "        mlp_model = MLPRegressor(**params)\n",
    "    \n",
    "        for train_index, validation_index in kf.split(X_train):\n",
    "            \n",
    "            X_train_kfold, X_val_kfold = X_train[train_index], X_train[validation_index]\n",
    "            Y_train_kfold, Y_val_kfold = Y_train[train_index], Y_train[validation_index]\n",
    "    \n",
    "            # -- data scaling: standardize features with respect to the current folds\n",
    "            scaler_kfold = preprocessing.StandardScaler().fit(X_train_kfold)\n",
    "            X_train_kfold_scaled = scaler_kfold.transform(X_train_kfold)\n",
    "            X_val_kfold_scaled = scaler_kfold.transform(X_val_kfold)\n",
    "              \n",
    "            # -- learn the model using the training data from the k-fold\n",
    "            mlp_model.fit(X_train_kfold_scaled, Y_train_kfold)\n",
    "            \n",
    "            \n",
    "            # -- incremental mean\n",
    "            err_train_kfold[i] += (1 - mlp_model.score(X_train_kfold_scaled, Y_train_kfold))\n",
    "            err_val_kfold[i] += (1 - mlp_model.score(X_val_kfold_scaled, Y_val_kfold))\n",
    "    \n",
    "    \n",
    "    # -- compute the mean => estimate of validation losses and errors for each lam\n",
    "    err_train_kfold /= num_folds\n",
    "    err_val_kfold /= num_folds\n",
    "    \n",
    "    # -- choose the regularization parameter that minimizes the loss\n",
    "    print('\\n---\\n')\n",
    "    best_param = param_list[np.argmin(err_val_kfold)]\n",
    "    print('Best value of the parameters:', best_param)\n",
    "    print('Min validation error:', np.min(err_val_kfold))\n",
    "\n",
    "    return best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- obtain the best paramaters by running k_fold_cross_validation on training data\n",
    "best_param = k_fold_cross_validation(X_train_scaled, Y_train, random_state = numero_di_matricola)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that with a smaller number of iterations we had a larger error on training set but a smaller error on validation data -> \"early stopping is a form of regularization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- let's train the model with best_param on train and validation\n",
    "final_model = MLPRegressor(**best_param)\n",
    "final_model.fit(X_train_and_val_scaled, Y_train_and_val)\n",
    "training_error = 1.0 - final_model.score(X_train_and_val_scaled, Y_train_and_val)\n",
    "print(\"Training error of best model: \", training_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- let's compute the test error\n",
    "test_error = 1.0 - final_model.score(X_test_scaled, Y_test)\n",
    "print(\"Test error of best model: \", test_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
